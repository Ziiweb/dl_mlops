{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (80.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: matplotlib in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de nuestro CVAE. Recordamos que el batch size es el numero de imágenes procesadas juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración\n",
    "batch_size = 128\n",
    "latent_dim = 16  # Dimensión del espacio latente\n",
    "num_classes = 10  # Dígitos del 0 al 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos los datos de entrenamiento (train=True) y prueba (train=False) del dataset MNIST, y transformamos las imagenes de los números a tensores. Los tensores tendrán 3 dimensiones: canal (1 por ser una imagen en escala de grises), alto (28) y ancho (28).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar MNIST con transformación a tensores\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos los datos de entrenamiento y test a una estructura DataLoader, pasando como parametro el batch size para procesar el dataset por lotes. De esa manera los tensores pasan de 3 a 4 dimensiones (B, C, H, W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "#### Encoder de CVAE\n",
    "\n",
    "El objetivo del encoder es transformar una imagen (28x28) y su etiqueta (10) en un espacio latente de dimension \"laten_dim\" (16 en este caso). En el espacio latente cada dato de entrada (imagen + etiqueta) está representado (mapeado) con una distribución probabilística, compuesto de la media y la varianza del dato de entrada.\n",
    "\n",
    "**Método init()**:\n",
    "\n",
    "En la función init() definimos los elementos que formarán parte del encoder. Como se puede ver, esos elementos son:\n",
    "* fc1: una capa lineal de 794 neuronas de entrada y 400 neuronas de salida. La razón de las 794 neuronas es que necesitamos introducir en ellas las imágenes y su etiqueta correspondiente, ambas en forma de tensor. El número 794 es el resultado de sumar 28*28 (768 = número de pixels de la imagen aplanada) y 10 (número de clases, del 0 al 9). El número de salidas de esa primrea capa lineal, 400, es porque la entrada del las capas fc2_mean y fc2_logvar es 400 también.\n",
    "\n",
    "* fc2_mean: capa que calcula la **media** (centro) de cada uno de los datos de entrada (imagen + etiqueta).\n",
    "\n",
    "* fc2_logvar: capa que calcula la **varianza** (centro) de cada uno de los datos de entrada (imagen + etiqueta).\n",
    "\n",
    "**Método forward()**:\n",
    "\n",
    "En un modelo CVAE, la entrada de cada uno de los datos tiene que llevar asociada su etiqueta correspondiente. Es decir al introducir en el modelo la imagen de un número 5, tendremos que añadir su etiqueta 5. La etiqueta será un tensor pyTorch, que se introduce en el encoder junto con la imagen. Por ejemplo, la etiqueta del número 5 es un tensor de 10 valores tal que [0, 0, 0, 0, 0, 1, 0, 0, 0, 0].\n",
    "\n",
    "Recordemos que en una celda previa, las imágenes las hemos convertido a tensores pyTorch, de la misma manera actuaremos con las etiquetas, es decir las convertiremos a tensores. De esta manera, al ser las dos elementos tensores, podremos concatenarlos para pasarlos a fc1 juntos. Dicha operación la realizamos a través de la función de pyTorch \"torch.cat()\". Para la concatenación, es necesario que ambos elementos tengan el mismo número de dimensiones, si no, dara error. Así que será necesario aplanar la imagen para que pasé de de 3 a 2 dimensiones al igual que la etiqueta.\n",
    "\n",
    "Al final del método calculamos la media y la varianza del vector y los devolvemos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28 + num_classes, 400) # images are 28x28\n",
    "\n",
    "        print(num_classes)\n",
    "\n",
    "        self.fc2_mean = nn.Linear(400, z_dim)\n",
    "        self.fc2_logvar = nn.Linear(400, z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # Flatten the 28x28 images so the concatenation in the next line works correctly. Otherwise, \n",
    "        # the concatenation would not work as x (image converted to PyTorch tensor) has dimension\n",
    "        # [batch, 28, 28] and label (classes converted to PyTorch tensor) has dimension\n",
    "        # [batch, 10].\n",
    "        x = x.view(-1, 28*28)  \n",
    "\n",
    "        x_label = torch.cat([x, label], dim=-1) # concatenamos la imagen y su etiqueta y el resultado lo asignmos a la variable \"x_label\" (\"x\" por la imagen y \"label\" por la etiqueta).\n",
    "        \n",
    "        x_label_relu = self.relu(self.fc1(x_label))\n",
    "\n",
    "        z_mean = self.fc2_mean(x_label_relu)\n",
    "        z_logvar = self.fc2_logvar(x_label_relu)\n",
    "        \n",
    "        return z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder de CVAE\n",
    "\n",
    "Esto es el código del decoder CVAE, que recibe como entrada un vector proveniente del espacio latente \"z\" (dimensión 16 en este caso) y una etiqueta de clase (número a partir del cual queremos que se genere una imagen), y genera una imagen reconstruida.\n",
    "\n",
    "**Método init()**\n",
    "\n",
    "* fc1 (convierte vectores de 26 dimensiones a vectores de 400 dimensiones): \n",
    "    * entrada:  26 neuronas, para poder recibir vectores de 26 elementos, ya que concatenamos vectores del espacio latente (16) y vectores que representan el número del que se quieren generar imágenes.\n",
    "    * salida: 400 neuronas. Se elige 400 porque suele ser el valor que mejores resultados da. \n",
    "\n",
    "* fc2 (convierte vectores de 400 dimensiones a vectores de 784 dimensiones): \n",
    "    * entrada: 400 neuronas\n",
    "    * salida: 784 neuronas ya que las imagenes que queremos generar son de 28x28.\n",
    "    \n",
    "* sigmoid: convierte vectores de 784 elementos a imágenes de tamaño 28x28.\n",
    "\n",
    "**Método forward()**\n",
    "\n",
    "* 1. Concatenamos el vector que viene del espacio latente con el vector asociado a la etiqueta del número que queremos generar una imagen.\n",
    "* 2. Lo introducimos en fc1.\n",
    "* 3. Lo pasamos por la capa de activación relu.\n",
    "* 4. Lo introducimos en fc2.\n",
    "* 5. Lo pasamos por la capa sigmoide para generar la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim):\n",
    "    \n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(z_dim + num_classes, 400)  # z_dim + num_classes (para la etiqueta)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(400, 28*28)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, label):\n",
    "\n",
    "        z_label = torch.cat([z, label], dim=-1)  # Concatenamos z con la etiqueta\n",
    "        \n",
    "        x = self.fc1(z_label)\n",
    "        \n",
    "        # Le he metido entre las capas lineales una Relu porque Pablo me ha dicho que las capas lineales\n",
    "        # sin \"nada\" entre medias es como si no hiciesemos \"nada\". Es decir, da lo mismo si pones 1 o 100 capas\n",
    "        # lineales si no pones \"nada\" entre medias.\n",
    "        x = self.relu(x) \n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x).view(-1, 28, 28)  # Reconstrucción de la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE\n",
    "\n",
    "Aquí implementamos el modelo generativo condicional.\n",
    "\n",
    "**Método init()**\n",
    "\n",
    "1. Creamos una instancia del encoder.\n",
    "2. Creamos una instancia del decoder.\n",
    "\n",
    "**Método reparameterize()**\n",
    "\n",
    "Este método se llama entre el encoder y el decoder, es decir, toma las salidas del encoder (media y varianza) y devuelve un vector del espacio latente que se pasa al decoder.\n",
    "\n",
    "En su interior se realiza el llamado \"reparameterization trick\".\n",
    "\n",
    "**Método forward()**\n",
    "\n",
    "1. Llamamo al encoder pasandole la imagen y su correspondiente etiqueta.\n",
    "2. Llamamo al reparameterize() para hacer el \"reparameterization trick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(z_dim)\n",
    "\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mean + eps * std\n",
    "\n",
    "\n",
    "    def forward(self, x, label):\n",
    "\n",
    "        z_mean, z_logvar = self.encoder(x, label)\n",
    " \n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    " \n",
    "        reconstructed_x = self.decoder(z, label)\n",
    " \n",
    "        return reconstructed_x, z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de perdida**\n",
    "\n",
    "* BCE (binary cross-entropy): recibe como parametros la imagen real y la reconstruida para medir cuanto se diferencian. Se suele usar para imágenes binarias (blanco/negro) como es nuestro caso, además de en problemas de clasificación binaria, de ahí lo del nombre (BINARY cros-entropy).\n",
    "\n",
    "* KL (divergencia Kullback-Leibler): mide como de diferentes son dos probabilidades de distribución. En CVAEs permite asegurar que el espacio latente sigue una distribución normal, ayudando al model a generar salidas con sentido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(reconstructed_x, x, z_mean, z_logvar):\n",
    "\n",
    "    BCE = nn.BCELoss(reduction='sum')   (reconstructed_x.view(-1, 28*28)   ,    x.view(-1, 28*28))\n",
    "\n",
    "    # Cálculo de la divergencia KL\n",
    "    KL = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "\n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la GPU para realizar el entrenamiento del modelo.\n",
    "\n",
    "Instanciamos el model y se los pasamos a la GPU.\n",
    "\n",
    "Usamos Adam como optimizador para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CVAE(z_dim=latent_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento del modelo**\n",
    "\n",
    "Vamos a entrenar el modelo durante 50 epochs.\n",
    "\n",
    "En cada epoch calcularemos la funcion de perdida y la mostramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 162.58331987304686\n",
      "Epoch 2/5, Loss: 119.65887294921875\n",
      "Epoch 3/5, Loss: 112.91282361653646\n",
      "Epoch 4/5, Loss: 109.59543572591146\n",
      "Epoch 5/5, Loss: 107.50340895182292\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target_one_hot = torch.zeros(target.size(0), num_classes).to(device).scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_data, z_mean, z_logvar = model(data, target_one_hot)\n",
    "        loss = loss_function(reconstructed_data, data, z_mean, z_logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    labels = torch.tensor(6).to(device)  # Queremos generar el dígito \"8\"\n",
    "    labels_one_hot = torch.zeros(1, num_classes).to(device).scatter_(1, labels.view(-1, 1), 1)\n",
    "    z = torch.randn(1, latent_dim).to(device)  # Z aleatorio\n",
    "    generated_image = model.decoder(z, labels_one_hot)\n",
    "\n",
    "    plt.imshow(generated_image.cpu().squeeze().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"cvae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "artifact_dir = \"./artifacts\"\n",
    "os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "# Guardar solo los pesos (recomendado para producción)\n",
    "torch.save(model.state_dict(), os.path.join(artifact_dir, \"cvae_model_state_dict.pth\"))\n",
    "\n",
    "# Opcional: guardar los hiperparámetros o configuración\n",
    "config = {\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "import json\n",
    "with open(os.path.join(artifact_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
