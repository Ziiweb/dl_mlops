{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (80.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: matplotlib in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: wandb in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (0.19.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (8.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (6.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (2.28.0)\n",
      "Requirement already satisfied: setproctitle in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (80.4.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: nbformat in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.25.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/tirengarfio/Downloads/dl_mlops/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install wandb\n",
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de nuestro CVAE. Recordamos que el batch size es el numero de imágenes procesadas juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración\n",
    "batch_size = 128\n",
    "latent_dim = 16  # Dimensión del espacio latente\n",
    "num_classes = 10  # Dígitos del 0 al 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos los datos de entrenamiento (train=True) y prueba (train=False) del dataset MNIST, y transformamos las imagenes de los números a tensores. Los tensores tendrán 3 dimensiones: canal (1 por ser una imagen en escala de grises), alto (28) y ancho (28).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar MNIST con transformación a tensores\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos los datos de entrenamiento y test a una estructura DataLoader, pasando como parametro el batch size para procesar el dataset por lotes. De esa manera los tensores pasan de 3 a 4 dimensiones (B, C, H, W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "#### Encoder de CVAE\n",
    "\n",
    "El objetivo del encoder es transformar una imagen (28x28) y su etiqueta (10) en un espacio latente de dimension \"laten_dim\" (16 en este caso). En el espacio latente cada dato de entrada (imagen + etiqueta) está representado (mapeado) con una distribución probabilística, compuesto de la media y la varianza del dato de entrada.\n",
    "\n",
    "**Método init()**:\n",
    "\n",
    "En la función init() definimos los elementos que formarán parte del encoder. Como se puede ver, esos elementos son:\n",
    "* fc1: una capa lineal de 794 neuronas de entrada y 400 neuronas de salida. La razón de las 794 neuronas es que necesitamos introducir en ellas las imágenes y su etiqueta correspondiente, ambas en forma de tensor. El número 794 es el resultado de sumar 28*28 (768 = número de pixels de la imagen aplanada) y 10 (número de clases, del 0 al 9). El número de salidas de esa primrea capa lineal, 400, es porque la entrada del las capas fc2_mean y fc2_logvar es 400 también.\n",
    "\n",
    "* fc2_mean: capa que calcula la **media** (centro) de cada uno de los datos de entrada (imagen + etiqueta).\n",
    "\n",
    "* fc2_logvar: capa que calcula la **varianza** (centro) de cada uno de los datos de entrada (imagen + etiqueta).\n",
    "\n",
    "**Método forward()**:\n",
    "\n",
    "En un modelo CVAE, la entrada de cada uno de los datos tiene que llevar asociada su etiqueta correspondiente. Es decir al introducir en el modelo la imagen de un número 5, tendremos que añadir su etiqueta 5. La etiqueta será un tensor pyTorch, que se introduce en el encoder junto con la imagen. Por ejemplo, la etiqueta del número 5 es un tensor de 10 valores tal que [0, 0, 0, 0, 0, 1, 0, 0, 0, 0].\n",
    "\n",
    "Recordemos que en una celda previa, las imágenes las hemos convertido a tensores pyTorch, de la misma manera actuaremos con las etiquetas, es decir las convertiremos a tensores. De esta manera, al ser las dos elementos tensores, podremos concatenarlos para pasarlos a fc1 juntos. Dicha operación la realizamos a través de la función de pyTorch \"torch.cat()\". Para la concatenación, es necesario que ambos elementos tengan el mismo número de dimensiones, si no, dara error. Así que será necesario aplanar la imagen para que pasé de de 3 a 2 dimensiones al igual que la etiqueta.\n",
    "\n",
    "Al final del método calculamos la media y la varianza del vector y los devolvemos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28 + num_classes, 400) # images are 28x28\n",
    "\n",
    "        print(num_classes)\n",
    "\n",
    "        self.fc2_mean = nn.Linear(400, z_dim)\n",
    "        self.fc2_logvar = nn.Linear(400, z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # Flatten the 28x28 images so the concatenation in the next line works correctly. Otherwise, \n",
    "        # the concatenation would not work as x (image converted to PyTorch tensor) has dimension\n",
    "        # [batch, 28, 28] and label (classes converted to PyTorch tensor) has dimension\n",
    "        # [batch, 10].\n",
    "        x = x.view(-1, 28*28)  \n",
    "\n",
    "        x_label = torch.cat([x, label], dim=-1) # concatenamos la imagen y su etiqueta y el resultado lo asignmos a la variable \"x_label\" (\"x\" por la imagen y \"label\" por la etiqueta).\n",
    "        \n",
    "        x_label_relu = self.relu(self.fc1(x_label))\n",
    "\n",
    "        z_mean = self.fc2_mean(x_label_relu)\n",
    "        z_logvar = self.fc2_logvar(x_label_relu)\n",
    "        \n",
    "        return z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder de CVAE\n",
    "\n",
    "Esto es el código del decoder CVAE, que recibe como entrada un vector proveniente del espacio latente \"z\" (dimensión 16 en este caso) y una etiqueta de clase (número a partir del cual queremos que se genere una imagen), y genera una imagen reconstruida.\n",
    "\n",
    "**Método init()**\n",
    "\n",
    "* fc1 (convierte vectores de 26 dimensiones a vectores de 400 dimensiones): \n",
    "    * entrada:  26 neuronas, para poder recibir vectores de 26 elementos, ya que concatenamos vectores del espacio latente (16) y vectores que representan el número del que se quieren generar imágenes.\n",
    "    * salida: 400 neuronas. Se elige 400 porque suele ser el valor que mejores resultados da. \n",
    "\n",
    "* fc2 (convierte vectores de 400 dimensiones a vectores de 784 dimensiones): \n",
    "    * entrada: 400 neuronas\n",
    "    * salida: 784 neuronas ya que las imagenes que queremos generar son de 28x28.\n",
    "    \n",
    "* sigmoid: convierte vectores de 784 elementos a imágenes de tamaño 28x28.\n",
    "\n",
    "**Método forward()**\n",
    "\n",
    "* 1. Concatenamos el vector que viene del espacio latente con el vector asociado a la etiqueta del número que queremos generar una imagen.\n",
    "* 2. Lo introducimos en fc1.\n",
    "* 3. Lo pasamos por la capa de activación relu.\n",
    "* 4. Lo introducimos en fc2.\n",
    "* 5. Lo pasamos por la capa sigmoide para generar la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim):\n",
    "    \n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(z_dim + num_classes, 400)  # z_dim + num_classes (para la etiqueta)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(400, 28*28)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, label):\n",
    "\n",
    "        z_label = torch.cat([z, label], dim=-1)  # Concatenamos z con la etiqueta\n",
    "        \n",
    "        x = self.fc1(z_label)\n",
    "        \n",
    "        # Le he metido entre las capas lineales una Relu porque Pablo me ha dicho que las capas lineales\n",
    "        # sin \"nada\" entre medias es como si no hiciesemos \"nada\". Es decir, da lo mismo si pones 1 o 100 capas\n",
    "        # lineales si no pones \"nada\" entre medias.\n",
    "        x = self.relu(x) \n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x).view(-1, 28, 28)  # Reconstrucción de la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE\n",
    "\n",
    "Aquí implementamos el modelo generativo condicional.\n",
    "\n",
    "**Método init()**\n",
    "\n",
    "1. Creamos una instancia del encoder.\n",
    "2. Creamos una instancia del decoder.\n",
    "\n",
    "**Método reparameterize()**\n",
    "\n",
    "Este método se llama entre el encoder y el decoder, es decir, toma las salidas del encoder (media y varianza) y devuelve un vector del espacio latente que se pasa al decoder.\n",
    "\n",
    "En su interior se realiza el llamado \"reparameterization trick\".\n",
    "\n",
    "**Método forward()**\n",
    "\n",
    "1. Llamamo al encoder pasandole la imagen y su correspondiente etiqueta.\n",
    "2. Llamamo al reparameterize() para hacer el \"reparameterization trick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(z_dim)\n",
    "\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mean + eps * std\n",
    "\n",
    "\n",
    "    def forward(self, x, label):\n",
    "\n",
    "        z_mean, z_logvar = self.encoder(x, label)\n",
    " \n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    " \n",
    "        reconstructed_x = self.decoder(z, label)\n",
    " \n",
    "        return reconstructed_x, z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de perdida**\n",
    "\n",
    "* BCE (binary cross-entropy): recibe como parametros la imagen real y la reconstruida para medir cuanto se diferencian. Se suele usar para imágenes binarias (blanco/negro) como es nuestro caso, además de en problemas de clasificación binaria, de ahí lo del nombre (BINARY cros-entropy).\n",
    "\n",
    "* KL (divergencia Kullback-Leibler): mide como de diferentes son dos probabilidades de distribución. En CVAEs permite asegurar que el espacio latente sigue una distribución normal, ayudando al model a generar salidas con sentido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(reconstructed_x, x, z_mean, z_logvar):\n",
    "\n",
    "    BCE = nn.BCELoss(reduction='sum')   (reconstructed_x.view(-1, 28*28)   ,    x.view(-1, 28*28))\n",
    "\n",
    "    # Cálculo de la divergencia KL\n",
    "    KL = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "\n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la GPU para realizar el entrenamiento del modelo.\n",
    "\n",
    "Instanciamos el model y se los pasamos a la GPU.\n",
    "\n",
    "Usamos Adam como optimizador para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CVAE(z_dim=latent_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tirengarfio/Downloads/dl_mlops/project/api/wandb/run-20250521_125535-wa9rdh0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v' target=\"_blank\">experimento-run</a></strong> to <a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops' target=\"_blank\">https://wandb.ai/javiergarpe1979-upm/dl_mlops</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v' target=\"_blank\">https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7424ef4b2ed0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"dl_mlops\",  # Cambia por el nombre de tu proyecto en wandb\n",
    "    name=\"experimento-run\",  # Opcional, nombre para identificar este run\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_classes\": num_classes\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento del modelo**\n",
    "\n",
    "Vamos a entrenar el modelo durante 50 epochs.\n",
    "\n",
    "En cada epoch calcularemos la funcion de perdida y la mostramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 161.71962908528647, Validation Loss: 124.31904295654297\n",
      "Epoch 2/50, Train Loss: 119.0018258951823, Validation Loss: 113.70207608642578\n",
      "Epoch 3/50, Train Loss: 112.56391098632812, Validation Loss: 109.63032963867188\n",
      "Epoch 4/50, Train Loss: 109.38452677408854, Validation Loss: 107.35182543945312\n",
      "Epoch 5/50, Train Loss: 107.38840922851563, Validation Loss: 105.84895255126953\n",
      "Epoch 6/50, Train Loss: 105.931208203125, Validation Loss: 104.91117653808594\n",
      "Epoch 7/50, Train Loss: 104.97188168945313, Validation Loss: 103.75475905761719\n",
      "Epoch 8/50, Train Loss: 104.10673660481771, Validation Loss: 103.41005305175781\n",
      "Epoch 9/50, Train Loss: 103.40581975911458, Validation Loss: 102.74619940185546\n",
      "Epoch 10/50, Train Loss: 102.81933636067708, Validation Loss: 102.2346053100586\n",
      "Epoch 11/50, Train Loss: 102.3916180501302, Validation Loss: 101.59349655761719\n",
      "Epoch 12/50, Train Loss: 102.00138798828125, Validation Loss: 101.2787396118164\n",
      "Epoch 13/50, Train Loss: 101.57453149414063, Validation Loss: 101.03049720458985\n",
      "Epoch 14/50, Train Loss: 101.29232267252604, Validation Loss: 100.81056979980468\n",
      "Epoch 15/50, Train Loss: 100.958202734375, Validation Loss: 100.68207667236328\n",
      "Epoch 16/50, Train Loss: 100.74813072916666, Validation Loss: 100.43690927734374\n",
      "Epoch 17/50, Train Loss: 100.43453600260416, Validation Loss: 99.9868828125\n",
      "Epoch 18/50, Train Loss: 100.25617805989583, Validation Loss: 100.10846574707031\n",
      "Epoch 19/50, Train Loss: 100.05443330078126, Validation Loss: 100.0158376953125\n",
      "Epoch 20/50, Train Loss: 99.8223703125, Validation Loss: 99.66470540771485\n",
      "Epoch 21/50, Train Loss: 99.68647311197917, Validation Loss: 99.63344368896485\n",
      "Epoch 22/50, Train Loss: 99.5147298828125, Validation Loss: 99.69256776123046\n",
      "Epoch 23/50, Train Loss: 99.37860493164062, Validation Loss: 99.49649777832032\n",
      "Epoch 24/50, Train Loss: 99.21937482096354, Validation Loss: 99.22925856933594\n",
      "Epoch 25/50, Train Loss: 99.06712283528645, Validation Loss: 99.02993021240235\n",
      "Epoch 26/50, Train Loss: 98.92960266927084, Validation Loss: 99.09355556640625\n",
      "Epoch 27/50, Train Loss: 98.81662283528645, Validation Loss: 99.11974366455078\n",
      "Epoch 28/50, Train Loss: 98.68474174804687, Validation Loss: 98.84076573486328\n",
      "Epoch 29/50, Train Loss: 98.59627765299479, Validation Loss: 98.60953933105469\n",
      "Epoch 30/50, Train Loss: 98.51173217773437, Validation Loss: 98.5595076171875\n",
      "Epoch 31/50, Train Loss: 98.3870723470052, Validation Loss: 98.41543002929687\n",
      "Epoch 32/50, Train Loss: 98.2852259765625, Validation Loss: 98.45788314208984\n",
      "Epoch 33/50, Train Loss: 98.20870592447916, Validation Loss: 98.3386162109375\n",
      "Epoch 34/50, Train Loss: 98.09981284179688, Validation Loss: 98.32570844726563\n",
      "Epoch 35/50, Train Loss: 98.00783826497396, Validation Loss: 98.33714407958985\n",
      "Epoch 36/50, Train Loss: 97.9160368326823, Validation Loss: 97.94193397216797\n",
      "Epoch 37/50, Train Loss: 97.87068139648437, Validation Loss: 98.37187012939454\n",
      "Epoch 38/50, Train Loss: 97.78565481770833, Validation Loss: 97.99683388671875\n",
      "Epoch 39/50, Train Loss: 97.67689259440104, Validation Loss: 97.85410710449219\n",
      "Epoch 40/50, Train Loss: 97.57427345377604, Validation Loss: 97.88628284912109\n",
      "Epoch 41/50, Train Loss: 97.52147433268229, Validation Loss: 97.73006240234375\n",
      "Epoch 42/50, Train Loss: 97.50057006835938, Validation Loss: 97.94979434814454\n",
      "Epoch 43/50, Train Loss: 97.40652906901042, Validation Loss: 97.69902169189453\n",
      "Epoch 44/50, Train Loss: 97.3176063313802, Validation Loss: 97.54685686035157\n",
      "Epoch 45/50, Train Loss: 97.22035501302084, Validation Loss: 97.59852607421875\n",
      "Epoch 46/50, Train Loss: 97.21014010416667, Validation Loss: 97.62854719238281\n",
      "Epoch 47/50, Train Loss: 97.11616184895833, Validation Loss: 97.60512358398438\n",
      "Epoch 48/50, Train Loss: 97.10538989257813, Validation Loss: 97.40640447998047\n",
      "Epoch 49/50, Train Loss: 96.9921518717448, Validation Loss: 97.42956317138672\n",
      "Epoch 50/50, Train Loss: 96.95821368815105, Validation Loss: 97.37878529052735\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Entrenamiento\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target_one_hot = torch.zeros(target.size(0), num_classes).to(device).scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_data, z_mean, z_logvar = model(data, target_one_hot)\n",
    "        loss = loss_function(reconstructed_data, data, z_mean, z_logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:  # o tu dataloader de validación\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            target_one_hot = torch.zeros(target.size(0), num_classes).to(device).scatter_(1, target.view(-1, 1), 1)\n",
    "            \n",
    "            reconstructed_data, z_mean, z_logvar = model(data, target_one_hot)\n",
    "            loss = loss_function(reconstructed_data, data, z_mean, z_logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader.dataset)\n",
    "\n",
    "    # Loguear en wandb ambos losses\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs = []\n",
    "    for digit in range(6):  # dígitos 0 a 5\n",
    "        label = torch.tensor([digit]).to(device)\n",
    "        label_one_hot = torch.zeros(1, num_classes).to(device).scatter_(1, label.view(-1, 1), 1)\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        generated_image = model.decoder(z, label_one_hot)  # salida [1, 28, 28]\n",
    "        \n",
    "        # Guardar imagen para wandb\n",
    "        img_np = generated_image.cpu().squeeze().numpy()\n",
    "        imgs.append(wandb.Image(img_np, caption=f\"Digit: {digit}\"))\n",
    "\n",
    "    # Loguear todas las imágenes juntas\n",
    "    wandb.log({\"generated_digits\": imgs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"cvae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_loss</td><td>96.95821</td></tr><tr><td>val_loss</td><td>97.37879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experimento-run</strong> at: <a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v' target=\"_blank\">https://wandb.ai/javiergarpe1979-upm/dl_mlops/runs/wa9rdh0v</a><br> View project at: <a href='https://wandb.ai/javiergarpe1979-upm/dl_mlops' target=\"_blank\">https://wandb.ai/javiergarpe1979-upm/dl_mlops</a><br>Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250521_125535-wa9rdh0v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "artifact_dir = \"./artifacts\"\n",
    "os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "# Guardar solo los pesos (recomendado para producción)\n",
    "torch.save(model.state_dict(), os.path.join(artifact_dir, \"cvae_model_state_dict.pth\"))\n",
    "\n",
    "# Opcional: guardar los hiperparámetros o configuración\n",
    "config = {\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "import json\n",
    "with open(os.path.join(artifact_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
